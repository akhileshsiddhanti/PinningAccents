<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Pinning Accents: A Study on Accent Classifiers  by sratnam6</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Pinning Accents: A Study on Accent Classifiers </h1>
      <h2 class="project-tagline">CS 7641 Machine Learning Project: Akanksha, Akhilesh, Kathan, Sheryl</h2>
    </section>

    <section class="main-content" align="justify">
      <h3>
<a id="problem-motivation" class="anchor" href="#problem-motivation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Problem Motivation</h3>
<p> Accents can signify a speakerâ€™s ethnic identity, regardless of the context or language being spoken. Personal assistants like Siri and Alexa have reach every corner of the world, and it has become important that such systems become inclusive to diversity in accents. This is one of the many cases where accent recognition systems become crucial, and our project is a small step towards that.
 </p>
 <img class="siri" src="aSiriBig.jpg">
<h3>
<a id="data-description" class="anchor" href="#data-description" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Data Description</h3>
<p>We have used a subset of the Speech Accent Archive<a href="https://www.kaggle.com/rtatman/speech-accent-archive"> [1]</a>, which is provided for general access to everyone on <a href="https://www.kaggle.com/">Kaggle</a>. This dataset has native and non-native speakers of English all read the same English paragraph and are carefully recorded.This dataset allows us to compare the demographic and linguistic backgrounds of the speakers in order to determine which variables are key predictors of each accent. The speech accent archive demonstrates that accents are systematic rather than merely mistaken speech.</p>
Some fun facts about our dataset:
 <img class="data" src="data.png">
 <p> We chose a subset of this dataset by taking the top 3 accents that have the most number of samples since all the other accents have very few samples which will bias our model. The final dataset consists of the following languages and number of samples: English (578), Spanish (162) and Arabic (102). In order to maintain the ratio of each accent in the dataset, we take a random subset of 150 English audio samples.</p>
 <h3>
 <a id="approach" class="anchor" href="#approach" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Approach</h3>
<p>Literature corresponding to automatic speech recognition, accent classification <a href="http://cs229.stanford.edu/proj2017/final-reports/5244230.pdf">[2]</a> or animal and bird sound classification <a href="https://digitalcommons.usu.edu/cgi/viewcontent.cgi?article=8156&context=etd">[3]</a> suggest using coefficients of the Mel Frequency Cepstrum <a href="http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/">[4]</a> and similar spectograms to capture the changing frequencies in a much more robust and efficient way. We use the audio and music processing python package called <a href="https://librosa.github.io/">Librosa</a> to process the audio signals and extract these features.</p>
<img class="waveform" src="Waveform.png">
<p><b>Mel-frequency cepstral coefficients (MFCC)</b>: MFCC coefficients model the spectral energy distribution of the power spectrum of the audio and are obtained by using Fast Fourier Transform, Mel Scale filtering and discrete consine transform. We obtain 20 such coeffcients and average them for all the frames for a given audio sample to get feature vector of length 20. </p>
<img class="mfcc" src="MFCC.png">
<p><b>Delta MFCC</b>: Delta MFCC is calculated by taking a one-step difference of the MFCC coefficients along the time axis. We also then average them along the time-frames to get a 20 length feature vector. Delta MFCC help us in capturing the temporal relations of the MFCC coefficients. </p>

We also experimented with other features, but found that they do are not representative of speech waveforms, such as <b>Chroma STFT</b>, <b>Spectral Contrast</b>, <b>Tonnetz</b> and <b>Melspectrogram</b>.

<p>We train <b>multiple supervised and un-supervised machine learning models</b> in order to understand the effectiveness of different models for accent classification. The features are also <b>normalized</b> to have a mean of 0 and standard deviation of 1 before being used to train the models. The data is also split in to a <b>80%-20% train-test split</b> using stratified sampling to ensure equal distribution of each class in the train and test splits. The trained models along with their hyperparamters are described in next section.</p>
<h3>
<a id="Experiments" class="anchor" href="#methodology" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Experiments</h3>
<p> All these experiments are carried out with different sets of parameters and datasets. To start out with, we used the top 9 accents, with just MFCC as the features and found that the results are not at par with the previous works. Below is our workflow for obtaining the different subset of data and features for our experiments!</p>
<img class="process" src="process.png">
<h4> K-Means and Gaussian Misture Model clustering </h4>
<p>We used K-Means to cluster the speech samples, and visualize the structure of data samples. We use the sklearn KMeans library, and used Principal Component Analysis on the flattened MFCC and Delta MFCC features. All the plottings were done using Linear Discriminant Analysis(LDA).</p>
<p> The clustering on top 3 samples, after using PCA and plotting on predicted labels looks like the following. </p>
<img class="kpca" src="kmeans_pca.png">
<p>Although, there are well-defined clusters for the predicted labels, the actual label plotting shows that the data is very spread out, and the clusering model is trying to cluster on only very specific parameters. This shows that our dataset has very minimal structure, which is captured on a very high level with clustering.</p>
<img class="ktrue" src="kmeans_true.png">
<p> Gaussian Mixture Model gives very similar results, reinforcing the minimal definition of structure of our data, as shown below. Left image is visualization with predicted labels, and right image is on true labels.</p>
<div class="gmm">
  <img class="gmmtrue" src="gmm_pca.png">
  <img class="gmmtrue" src="gmm_true.png">
</div>
<h4> Support Vector Classifier </h4>
<p> One-vs-all classification is used for a multi-class problem like this. We use a Radial Basis Function (RBF) kernel for the kernel transformation. The scores of our model is visualized as shown.</p>
<img class="svc" src="SVC.png">
<h4> Logistic Regression </h4>
<p> It is a variant of the linear regression but is uses the log-probability of a given class as the dependent variable. The coefficients of the model can be analyzed to undertand the magnitude and direction of impact of a given feature in classification.
The decision boundaries of top 3 acccents with the distribution of test data is shown below.</p>
<img class="logreg" src="LogReg.png">
<h4> K-Nearest Neighbours </h4>
<p>Assign an audio sample the majority class that exists among its 5 closest neighbouring audio samples in the Euclidean space. Below is the graph for ther classifier, showing neighborhoods and classificsation.</p>
<img class="knn" src="KNN.png">
<h4>Naive Bayes</h4>
<p>The likelihood of the features is assumed to be a Gaussian distribution and training involves learning the model paramters of mean and sigma for each Gaussian.</p>
<img class="gnb" src="GNB.png">
<h4> Decision Tree Classifier </h4>
<p> Predict class of a given audio sample by learning simple decision rules from training data. We use the deafult paramters while training the model. The decision tree of our data is visualized as below.</p>
<img class="dtree" src="decision_tree.png">
<h4> Random Forest Classifier </h4>
<p>A group of decision trees, each fit on a subset of the data and use averaging to improve to improve accuracy and control over-fitting. We use the default value of 10 estimators (decision trees).</p>
<h4> Convolutional Neural Network </h4>
<p> We convert the MFCC features for a give audio signal to an image and train a Convolutional Neural Network on top of it. Deep Learning has seen an increased usage in a variety of tasks and we hope that the CNN will be able to capture subtle differences and changes in the MFCC coefficients over time. Trained a shallow CNN on the MFCC images using a categorical cross entropy loss to capture the temporal changes in the MFCC coefficients.</p>
<h3>
<a id="analysis" class="anchor" href="#analysis" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Results and Analysis</h3>
<p>Results about accuracy and analysis.</p>

<p> (1) Add tables for accuracy of each supervised approach (overall and for each langauge). (2) PCA/GMM plots for unsuperivsed. (3) Feature importance analysis for supervised approaches. (graphs) (4) Deep learning model selection analysis. </p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.gatech.edu/sratnam6/sratnam6.github.io">Pinning Accents: A Study on Accent Classifiers </a> is maintained by <a href="https://github.gatech.edu/sratnam6">sratnam6</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>


  </body>
</html>
