<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Pinning Accents: A Study on Accent Classifiers  by sratnam6</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Pinning Accents: A Study on Accent Classifiers </h1>
      <h2 class="project-tagline">CS 7641 Machine Learning Project: Akanksha, Akhilesh, Kathan, Sheryl</h2>
    </section>

    <section class="main-content" align="justify">
      <h3>
<a id="problem-motivation" class="anchor" href="#problem-motivation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Problem Motivation</h3>
<p> Accents can signify a speakerâ€™s ethnic identity, regardless of the context or language being spoken. Personal assistants like Siri and Alexa have reach every corner of the world, and it has become important that such systems become inclusive to diversity in accents. This is one of the many cases where accent recognition systems become crucial, and our project is a small step towards that.
 </p>
 <img class="siri" src="aSiriBig.jpg">
<h3>
<a id="data-description" class="anchor" href="#data-description" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Data Description</h3>
<p>We have used a subset of the Speech Accent Archive<a href="https://www.kaggle.com/rtatman/speech-accent-archive"> [1]</a>, which is provided for general access to everyone on <a href="https://www.kaggle.com/">Kaggle</a>. This dataset has native and non-native speakers of English all read the same English paragraph and are carefully recorded.This dataset allows us to compare the demographic and linguistic backgrounds of the speakers in order to determine which variables are key predictors of each accent. The speech accent archive demonstrates that accents are systematic rather than merely mistaken speech.</p>
Some fun facts about our dataset:
 <img class="data" src="data.png">
 <p> We chose a subset of this dataset by taking the top 3 accents that have the most number of samples since all the other accents have very few samples which will bias our model. The final dataset consists of the following languages and number of samples: English (578), Spanish (162) and Arabic (102). In order to maintain the ratio of each accent in the dataset, we take a random subset of 150 English audio samples.</p>
 <h3>
 <a id="approach" class="anchor" href="#approach" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Approach</h3>
<p>Literature corresponding to automatic speech recognition, accent classification <a href="http://cs229.stanford.edu/proj2017/final-reports/5244230.pdf">[2]</a> or animal and bird sound classification <a href="https://digitalcommons.usu.edu/cgi/viewcontent.cgi?article=8156&context=etd">[3]</a> suggest using coefficients of the Mel Frequency Cepstrum <a href="http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/">[4]</a> and similar spectograms to capture the changing frequencies in a much more robust and efficient way. We use the audio and music processing python package called <a href="https://librosa.github.io/">Librosa</a> to process the audio signals and extract these features.</p>
<img class="waveform" src="Waveform.png">
<p><b>Mel-frequency cepstral coefficients (MFCC)</b>: MFCC coefficients model the spectral energy distribution of the power spectrum of the audio and are obtained by using Fast Fourier Transform, Mel Scale filtering and discrete consine transform. We obtain 20 such coeffcients and average them for all the frames for a given audio sample to get feature vector of length 20. </p>
<img class="mfcc" src="MFCC.png">
<p><b>Delta MFCC</b>: Delta MFCC is calculated by taking a one-step difference of the MFCC coefficients along the time axis. We also then average them along the time-frames to get a 20 length feature vector. Delta MFCC help us in capturing the temporal relations of the MFCC coefficients. </p>

We also experimented with other features, but found that they do are not representative of speech waveforms, such as <b>Chroma STFT</b>, <b>Spectral Contrast</b>, <b>Tonnetz</b> and <b>Melspectrogram</b>.

<p>We train <b>multiple supervised and un-supervised machine learning models</b> in order to understand the effectiveness of different models for accent classification. The features are also <b>normalized</b> to have a mean of 0 and standard deviation of 1 before being used to train the models. The data is also split in to a <b>80%-20% train-test split</b> using stratified sampling to ensure equal distribution of each class in the train and test splits. The trained models along with their hyperparamters are described in next section.</p>
<h3>
<a id="Experiments" class="anchor" href="#methodology" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Experiments</h3>
<p> All these experiments are carried out with different sets of parameters and datasets. To start out with, we used the top 9 accents, with just MFCC as the features and found that the results are not at par with the previous works. Below is our workflow for obtaining the different subset of data and features for our experiments!</p>
<img class="process" src="process.png">
<h4> K-Means and Gaussian Misture Model clustering </h4>
<p>We used K-Means to cluster the speech samples, and visualize the structure of data samples. We use the sklearn KMeans library, and used Principal Component Analysis on the flattened MFCC and Delta MFCC features. All the plottings were done using Linear Discriminant Analysis(LDA).</p>
<p> The clustering on top 3 samples, after using PCA and plotting on predicted labels looks like the following. </p>
<img class="kpca" src="kmeans_pca.png">
<p>Although, there are well-defined clusters for the predicted labels, the actual label plotting shows that the data is very spread out, and the clusering model is trying to cluster on only very specific parameters. This shows that our dataset has very minimal structure, which is captured on a very high level with clustering.</p>
<img class="ktrue" src="kmeans_true.png">
<p> Gaussian Mixture Model gives very similar results, reinforcing the minimal definition of structure of our data, as shown below. Left image is visualization with predicted labels, and right image is on true labels.</p>
<div class="gmm">
  <img class="gmmtrue" src="gmm_pca.png">
  <img class="gmmtrue" src="gmm_true.png">
</div>

<h4> Support Vector Classifier </h4>
<p> One-vs-all classification is used for a multi-class problem like this. We use a Radial Basis Function (RBF) kernel for the kernel transformation. The scores of our model is visualized as shown.</p>
<img class="svc" src="SVC.png">

<h4> Logistic Regression </h4>
<p> Logistic Regression is a variant of the linear regression but it uses the log-probability of a given class as the dependent variable. The regression coefficients can be analyzed to understand the magnitude and direction of impact of a given feature in classification. The decision boundaries of top 3 accents with the distribution of test data is shown below.As we can see in the below plot, the <b>English accents from Spanish and Arabic are linearly separable</b>, while native <b>English accent is not so linearly classifiable</b>.
</p>
<img class="logreg" src="logistic_regression_plot.png">

<h4> K-Nearest Neighbours </h4>
<p>We assign an audio sample the majority class that exists among its 5 closest neighbouring audio samples in the Euclidean space. Below is the graph for ther classifier, showing neighborhoods and classificsation. Since the data is cluttered, the model is <b>not able to classify the central points effectively</b>, but is <b>capable of drawing boundaries for the points on the edges</b> as depicted in the below figure.</p>
<img class="knn" src="KNN.png">

<h4>Naive Bayes</h4>
<p>The underlying likelihood of the features is assumed to be a Gaussian distribution and training on the dataset involves learning the model paramters which are the mean and sigma for each Gaussian. Naive Bayes assumes conditional independence which is a strong assumption for this problem since the features used for training are interrelated. Since the <b>data has large overlays, the underlying Gaussian distributions are not learned properly</b>.</p>
<img class="gnb" src="GNB.png">

<h4> Decision Tree Classifier </h4>
<p> Decision trees involve predicting the class of a given audio sample by learning simple decision rules from training data. We use the deafult paramters of the model available in the scikit-learn library while training the model. The decision tree of our top 3 accents MFCC+Delta MFCC data is visualized as below.</p>
<img class="dtree" src="decision_tree.png">

<h4> Random Forest Classifier </h4>
<p>The random forest is a group of decision trees, each fit on a subset of the data and use averaging to improve to improve accuracy and control over-fitting. We use the default value of 10 estimators (decision trees) and use 10-fold cross validation to obtain the best set of trees in the forest.</p>

<h4> Convolutional Neural Network </h4>
<p> We convert the MFCC features for a give audio signal to an image and train a Convolutional Neural Network on top of it. Deep Learning has seen an increased usage in a variety of tasks and we hope that the CNN will be able to capture subtle differences and changes in the MFCC coefficients over time. We trained a <b>4 layer CNN of 32 3*3 filters at each layer and a 128 sized fully-connected layer using a categorical cross entropy loss</b>.</p>
<h3>
<a id="analysis" class="anchor" href="#analysis" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Results and Analysis</h3>
<h4> Clustering performance </h4>
<p>We observed that clustering algorithms deduce the best structure with the MFCC+Delta MFCC features, as shown in previous section. We have used PCA on the feature set with two principal components.</p>
<br>
<table>
  <caption> Clustering evaluation</caption>
  <tr>
    <th>Clustering Algorithm</th>
    <th>V-Measure</th>
    <th>Recovered variance for first PCA principal component</th>
  </tr>
  <tr>
    <td>K-Means</td>
    <td>0.034375228348756864</td>
    <td>0.98633235</td>
  </tr>
  <tr>
    <td>Gaussian Mixture Model</td>
    <td>0.03646988561726508</td>
    <td>0.98634451</td>
  </tr>
</table>
<p> Performance of both the algorithms is comparable, and it could be attributed to minimal structure in the data </p>
<h4> Accuracy results and analysis </h4>
<p>As mentioned in the previous section, we have experimented with a lot of feature combinations for our dataset, but found that a group of features that capture difference characteristics of the audio signal are representative of speech sample and give the best results. Here are the accuracy tables of <b>supervised learning models</b> for each of such combinations.</p>
<br>
<table>
  <caption> MFCC </caption>
  <tr>
    <th>Model</th>
    <th>Test Accuracy</th>
    <th>English Accuracy</th>
    <th>Spanish Accuracy</th>
    <th>Arabic Accuracy</th>
  </tr>
  <tr>
    <td>Support Vector Classifier</td>
    <td>0.51</td>
    <td>0.63</td>
    <td>0.52</td>
    <td>0.35</td>
  </tr>
  <tr>
    <td>Logistic Regression</td>
    <td>0.53</td>
    <td>0.67</td>
    <td>0.55</td>
    <td>0.3</td>
  </tr>
  <tr>
    <td>K- Nearest Neighbour</td>
    <td>0.51</td>
    <td>0.6</td>
    <td>0.49</td>
    <td>0.45</td>
  </tr>
  <tr>
    <td>Naive Bayes</td>
    <td>0.42</td>
    <td>0.6</td>
    <td>0.12</td>
    <td>0.65</td>
  </tr>
  <tr>
    <td>Decision Tree</td>
    <td>0.42</td>
    <td>0.4</td>
    <td>0.49</td>
    <td>0.35</td>
  </tr>
  <tr>
    <td>Random Forest</td>
    <td>0.43</td>
    <td>0.5</td>
    <td>0.42</td>
    <td>0.35</td>
  </tr>
</table>

<br>
<table>
  <caption> MFCC+Delta MFCC</caption>
  <tr>
    <th>Model</th>
    <th>Test Accuracy</th>
    <th>English Accuracy</th>
    <th>Spanish Accuracy</th>
    <th>Arabic Accuracy</th>
  </tr>
  <tr>
    <td>Support Vector Classifier</td>
    <td>0.53</td>
    <td>0.67</td>
    <td>0.54</td>
    <td>0.33</td>
  </tr>
  <tr>
    <td>Logistic Regression</td>
    <td>0.482</td>
    <td>0.567</td>
    <td>0.489</td>
    <td>0.35</td>
  </tr>
  <tr>
    <td>K- Nearest Neighbour</td>
    <td>0.506</td>
    <td>0.4</td>
    <td>0.576</td>
    <td>0.55</td>
  </tr>
  <tr>
    <td>Naive Bayes</td>
    <td>0.47</td>
    <td>0.6</td>
    <td>0.21</td>
    <td>0.7</td>
  </tr>
  <tr>
    <td>Decision Tree</td>
    <td>0.45</td>
    <td>0.53</td>
    <td>0.42</td>
    <td>0.35</td>
  </tr>
  <tr>
    <td>Random Forest</td>
    <td>0.45</td>
    <td>0.53</td>
    <td>0.45</td>
    <td>0.3</td>
  </tr>
</table>
<br>
<table>
  <caption> All features </caption>
  <tr>
    <th>Model</th>
    <th>Test Accuracy</th>
    <th>English Accuracy</th>
    <th>Spanish Accuracy</th>
    <th>Arabic Accuracy</th>
  </tr>
  <tr>
    <td>Support Vector Classifier</td>
    <td>0.55</td>
    <td>0.67</td>
    <td>0.58</td>
    <td>0.35</td>
  </tr>
  <tr>
    <td>Logistic Regression</td>
    <td>0.59</td>
    <td>0.63</td>
    <td>0.58</td>
    <td>0.55</td>
  </tr>
  <tr>
    <td>K- Nearest Neighbour</td>
    <td>0.6</td>
    <td>0.6</td>
    <td>0.67</td>
    <td>0.5</td>
  </tr>
  <tr>
    <td>Naive Bayes</td>
    <td>0.37</td>
    <td>0.4</td>
    <td>0.06</td>
    <td>0.85</td>
  </tr>
  <tr>
    <td>Decision Tree</td>
    <td>0.59</td>
    <td>0.57</td>
    <td>0.63</td>
    <td>0.55</td>
  </tr>
  <tr>
    <td>Random Forest</td>
    <td>0.55</td>
    <td>0.73</td>
    <td>0.51</td>
    <td>0.35</td>
  </tr>
</table>

<p> According to these accuracy scores, we found that the <b>combination of all features gives us the best results except for Naive Bayes</b> since adding features did not improve the data separation, in general, across all the accent classes. </p>

<h4> Confusion Matrices for the best feature set </h4>
<p> Below are the confusion matrix for each supervised algorithm when trained on the combination of all features.</p>
<table width="500" border="1" cellpadding="5">
<tr>
  <td align="center" valign="center">

  <img class = "svmcm" src="svm_cm.png"/>
  </td>

  <td align="center" valign="center">
  <b>Support Vector Classifier</b>
  </td>
</tr>
<tr>
  <td align="center" valign="center">

  <img class = "svmcm" src="logreg_cm.png"/>
  </td>

  <td align="center" valign="center">
  <b>Logistic Regression</b>
  </td>
</tr>
<tr>
  <td align="center" valign="center">

  <img class = "svmcm" src="knn_cm.png"/>
  </td>

  <td align="center" valign="center">
  <b>K- Nearest Neighbour</b>
  </td>
</tr>
<tr>
  <td align="center" valign="center">

  <img class = "svmcm" src="naive_cm.png"/>
  </td>

  <td align="center" valign="center">
  <b>Naive Bayes</b>
  </td>
</tr>
<tr>
  <td align="center" valign="center">

  <img class = "svmcm" src="decision_cm.png"/>
  </td>

  <td align="center" valign="center">
  <b>Decision Tree</b>
  </td>
</tr>
<tr>
  <td align="center" valign="center">

  <img class = "svmcm" src="random_cm.png"/>
  </td>

  <td align="center" valign="center">
  <b>Random Forest</b>
  </td>
</tr>
</table>
<h3>
<a id="Experiments" class="anchor" href="#methodology" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>References</h3>
<a href="https://www.kaggle.com/rtatman/speech-accent-archive"> [1] </a>Weinberger, Steven. (2015). Speech Accent Archive. George Mason University. Retrieved from http://accent.gmu.edu <br>
<a href="http://cs229.stanford.edu/proj2017/final-reports/5244230.pdf">[2]</a> Sheng, L.M. (2017). Deep Learning Approach To Accent Classification. <br>
<a href="https://digitalcommons.usu.edu/cgi/viewcontent.cgi?article=8156&context=etd">[3]</a> Crypto (n.d.). Retrieved from http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/. <br>
<a href="http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/">[4] </a>Amlathe, P. (2018). Standard Machine Learning Techniques in Audio Beehive Monitoring: Classification of Audio Samples with Logistic Regression, K-Nearest Neighbor, Random Forest and Support Vector Machine.
    <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.gatech.edu/sratnam6/sratnam6.github.io">Pinning Accents: A Study on Accent Classifiers </a> is maintained by <a href="https://github.gatech.edu/sratnam6">sratnam6</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>


  </body>
</html>
